import tokenize
import hashlib
import csv
import unicodedata

EMOJI_NAMES = list(c['Name'] for c in
                   csv.DictReader(
                       open('./full-emoji-list.tsv'), delimiter='\t')
                   )


#  None
#  and
#  as
#  assert
#  continue
#  def
#  del
#  elif
#  else
#  except
#  finally
#  for
#  from
#  in
#  is
#  nonlocal
#  not
#  or
#  raise
#  return
#  try
#  while
#  with

REPLACEMENTS = {
    (tokenize.OP,   'â­'): '*',
    (tokenize.NAME, 'â”'): 'if',
    (tokenize.NAME, 'â”â©'): 'pass',
    (tokenize.NAME, 'â”ğŸŒ'): 'global',
    (tokenize.NAME, 'â”ğŸ’”'): 'break',
    (tokenize.NAME, 'â”ğŸ‘'): 'True',
    (tokenize.NAME, 'â”ğŸ‘'): 'False',
    (tokenize.NAME, 'ğŸ‡«ğŸ‡·'): 'yield',
    (tokenize.NAME, 'ğŸš«'): 'not',
    (tokenize.NAME, 'ğŸ‘'): 'lambda',
    (tokenize.NAME, 'ğŸ«'): 'class',
    (tokenize.NAME, 'ğŸ“¥'): 'import',
}


def replace_keywords(token_list):
    for t in token_list:
        if (t.type, t.string) in REPLACEMENTS:
            yield tokenize.TokenInfo(
                type=t.type,
                string=REPLACEMENTS[(t.type, t.string)],
                start=t.start,
                end=t.end,
                line=t.line
            )
        else:
            yield t


def is_emoji(c):
    return unicodedata.name(c) in EMOJI_NAMES


def replace_names(token_list):
    for t in token_list:
        if t.type == tokenize.NAME:
            if any(map(is_emoji, t.string)):
                yield tokenize.TokenInfo(
                    type=t.type,
                    string='_' +
                    hashlib.md5(t.string.encode('utf-8')).hexdigest(),
                    start=t.start,
                    end=t.end,
                    line=t.line
                )
            break
        yield t


def replace_emoji(token_list):
    token_list = replace_keywords(token_list)
    token_list = replace_names(token_list)
    return list(token_list)


token_list = [
    tokenize.TokenInfo(
        type=tokenize.AT, string='@', start=None, end=None, line=None),
    tokenize.TokenInfo(
        type=tokenize.OP, string='â­', start=None, end=None, line=None),
    tokenize.TokenInfo(
        type=tokenize.NAME, string='barğŸ«', start=None, end=None, line=None),
]
replace_emoji(token_list)
